{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "990cc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "There are four general steps for deep learning models:\n",
    "\n",
    "1.    Prepare the data\n",
    "\n",
    "2.    Build the model\n",
    "\n",
    "3.    Train the model\n",
    "\n",
    "4.    Analyze the model's results\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Today Lecture\n",
    "\n",
    "1. PyTorch import\n",
    "\n",
    "2. Creat fully connected Network\n",
    "\n",
    "3. Set device\n",
    "\n",
    "4. Hyperparameters\n",
    "\n",
    "5. load data\n",
    "\n",
    "6. Initalize NW\n",
    "\n",
    "7. Load and optimization\n",
    "\n",
    "8. Train NW\n",
    "\n",
    "9. Check accuracy on training and test to see how good our model\n",
    "\n",
    " \n",
    "\n",
    "Resources:\n",
    "\n",
    "1. https://aladdinpersson.medium.com/pytorch-neural-network-tutorial-7e871d6be7c4\n",
    "\n",
    " \n",
    "\n",
    "use the following code and do it for NN instead of ConvNet\n",
    "\n",
    "2. https://cs230.stanford.edu/blog/handsigns/\n",
    "\n",
    " \n",
    "\n",
    "3. #Deeplizard:\n",
    "\n",
    "#https://www.youtube.com/watch?v=v5cngxo4mIg&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG\n",
    "\n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "#First we need will need a couple of different packages\n",
    "\n",
    " \n",
    "import os  # Used to interact with folders and file paths\n",
    "\n",
    "from PIL import Image  # Python Imaging Library used to open image files\n",
    "\n",
    "from torchvision import transforms  # To apply transformations to images (like resizing, converting to tensors)\n",
    "\n",
    "import torch  #The top-level PyTorch package and tensor library.\n",
    "\n",
    "import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "\n",
    "import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "\n",
    "import torch.nn.functional as F # All functions that don't have any parameters, like relu etc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader  # For making custom datasets and loading them in batches \n",
    "                                                  # Gives easier dataset managment and creates mini batches\n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "#torch.utils.data:          Extract: Access data from source, and create minibatches\n",
    "\n",
    "torch.util.data.DataLoader 1.Extract data from source(and load provide its access) and creat minibatches\n",
    "\n",
    "torch.utils.data.Datasaet  2.If we make our owndataset then import ,and implement its abstract functions=>__getitem__() and __len__().\n",
    "\n",
    "'''\n",
    "\n",
    " \n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchvision.datasets as datasets # Has standard datasets we can import in a nice and easy way\n",
    "\n",
    "import torchvision.transforms as transforms # Perform transformations on dataset (convert numpy to tensor data)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "635232e4-3235-44e6-b301-7635d036fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both training and testing datasets\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir        # path to the folder that contains subfolders 0,1,2,...\n",
    "        self.transform = transform      # image transformation pipeline (like resize and convert to tensor)\n",
    "        self.image_paths = []           # list to store all image file paths\n",
    "        self.labels = []                # list to store the corresponding labels (0 to 9)\n",
    "\n",
    "        # go through each folder inside the root_dir\n",
    "        for label in os.listdir(root_dir):     # label = folder name = '0', '1', ..., '9'\n",
    "            label_path = os.path.join(root_dir, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for image_file in os.listdir(label_path):  # go through all images in that folder\n",
    "                    image_path = os.path.join(label_path, image_file)\n",
    "                    self.image_paths.append(image_path)    # full path to the image\n",
    "                    self.labels.append(int(label))         # label is the folder name, convert to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)  # number of samples in dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"L\")  # open image in grayscale\n",
    "        label = self.labels[idx]  # corresponding label (digit 0 to 9)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # apply transformations\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ff3b34e-7cb4-4860-9847-a205f8a6d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to apply to each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),   # Resize image to 64x64 pixels\n",
    "    transforms.ToTensor()          # Convert image to tensor format (needed for PyTorch)\n",
    "])\n",
    "\n",
    "# Load training and testing data using our custom class\n",
    "train_dataset = SignDataset(\"Sign-Language-Digits-Dataset/training_dataset\", transform=transform)\n",
    "test_dataset = SignDataset(\"Sign-Language-Digits-Dataset/test_dataset\", transform=transform)\n",
    "\n",
    "# Load data in batches using DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b99baa84-15ac-48de-bc77-d4931061fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(64*64, 128)   # input layer (flattened 64x64 image) to hidden\n",
    "        self.fc2 = nn.Linear(128, 64)      # hidden layer\n",
    "        self.fc3 = nn.Linear(64, 10)       # output layer: 10 classes for digits 0-9\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 64*64)   # flatten the image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)         # final scores (logits), no softmax needed\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f380eae-3433-4a9f-9f8c-9920f59393a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.3098\n",
      "Epoch 2 - Loss: 2.2455\n",
      "Epoch 3 - Loss: 2.0401\n",
      "Epoch 4 - Loss: 1.8132\n",
      "Epoch 5 - Loss: 1.7232\n",
      "Epoch 6 - Loss: 1.5249\n",
      "Epoch 7 - Loss: 1.4709\n",
      "Epoch 8 - Loss: 1.4259\n",
      "Epoch 9 - Loss: 1.2546\n",
      "Epoch 10 - Loss: 1.1763\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()  # Create the model\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer for training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Train for 10 passes through the data\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()            # Clear previous gradients\n",
    "        outputs = model(images)          # Forward pass\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "        loss.backward()                  # Backward pass\n",
    "        optimizer.step()                 # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70b14547-2aab-46d4-8fb0-6597e772e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.84%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients while testing\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
